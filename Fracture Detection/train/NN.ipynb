{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from enum import Enum\n",
    "import cv2\n",
    "from tensorflow.keras import backend as K # Importing Keras backend (by default it is Tensorflow)\n",
    "import gc\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras_adabound import AdaBound\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.layers import Input, Conv2D,BatchNormalization\n",
    "from tensorflow.keras.layers import Activation,SpatialDropout2D,AvgPool2D\n",
    "from tensorflow.keras.layers import MaxPool2D,Dropout,GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D,Flatten,Dropout,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow_addons.metrics import CohenKappa\n",
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "from tensorflow.keras.applications import ResNet152V2\n",
    "from tensorflow.keras.applications import DenseNet121,DenseNet169,DenseNet201\n",
    "from tensorflow.keras.models import load_model\n",
    "from numpy import dstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define seed number to have reproducible experiments.\n",
    "seed = 3352024\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_path(path):\n",
    "    '''\n",
    "    load MURA dataset\n",
    "    '''\n",
    "    dataset = [] \n",
    "    for body in os.listdir(path):\n",
    "        body_part = body\n",
    "        path_p = path+'/'+str(body)\n",
    "        for id_p in os.listdir(path_p):\n",
    "            patient_id = id_p\n",
    "            path_id = path_p+'/'+str(id_p)\n",
    "            for lab in os.listdir(path_id):\n",
    "                if lab.split('_')[-1]=='positive': \n",
    "                    label = 1 \n",
    "                elif lab.split('_')[-1]=='negative':\n",
    "                    label= 0\n",
    "                path_l = path_id+'/'+str(lab)\n",
    "                for img in os.listdir(path_l):  \n",
    "                    img_path = path_l + '/' + str(img)\n",
    "                    dataset.append(\n",
    "                        {\n",
    "                            'body_part': body_part,\n",
    "                            'patient_id': patient_id,\n",
    "                            'label': label,\n",
    "                            'img_path': img_path\n",
    "                        }\n",
    "                    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'MURA-v1.1/train'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train= load_path(path)\n",
    "df_train = pd.DataFrame(dataset_train)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = load_path(path = 'MURA-v1.1/valid')\n",
    "df_test = pd.DataFrame(dataset_test)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all=pd.concat([df_train,df_test])\n",
    "print(dataset_all.body_part.value_counts())\n",
    "print(dataset_all.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape[0]/dataset_all.shape[0])\n",
    "print(df_test.shape[0]/dataset_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train.body_part=='XR_WRIST'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_builder(\n",
    "        input_shape=(256, 256, 3),\n",
    "        starting_filters=32,\n",
    "        conv_layers=1,\n",
    "        conv_strides=(1, 1),\n",
    "        conv_kernel=(3, 3),\n",
    "        convs_per_layer=1,\n",
    "        batch_norm=False,\n",
    "        pooling=\"max\",\n",
    "        dropout=None,\n",
    "        pool_size=(2, 2),\n",
    "        pool_strides=(2, 2),\n",
    "        last_pooling=None,\n",
    "        spatial_dropout=None,\n",
    "        last_dropout=None,\n",
    "        numdense=1\n",
    "):\n",
    "    inputs = Input(\n",
    "        shape=input_shape,\n",
    "        name=\"input\"\n",
    "    )\n",
    "    x = inputs\n",
    "    for conv_level in range(conv_layers):\n",
    "        current_filters = starting_filters * (2 ** conv_level)\n",
    "        for conv_number in range(convs_per_layer):\n",
    "            x = Conv2D(\n",
    "                filters=current_filters,\n",
    "                kernel_size=conv_kernel,\n",
    "                strides=conv_strides,\n",
    "                name=f\"conv_{conv_level}_{conv_number}\",\n",
    "                padding='same'\n",
    "            )(x)\n",
    "            if batch_norm:\n",
    "                x = BatchNormalization(name=f\"bn_{conv_level}_{conv_number}\")(x)\n",
    "            x = Activation(\"relu\", name=f\"conv_{conv_level}_{conv_number}_relu\")(x)\n",
    "        if spatial_dropout:\n",
    "            x = SpatialDropout2D(spatial_dropout, name=f\"sp_dropout_{conv_level}\")(x)\n",
    "        if pooling == 'avg':\n",
    "            x = AvgPool2D(pool_size=pool_size, \n",
    "                          strides=pool_strides,\n",
    "                          name=f\"mp_{conv_level}\",\n",
    "                          padding='same')(x)\n",
    "        elif pooling == 'max':\n",
    "            x = MaxPool2D(pool_size=pool_size,\n",
    "                          strides=pool_strides,\n",
    "                          name=f\"mp_{conv_level}\",\n",
    "                          padding='same')(x)\n",
    "        if dropout:\n",
    "            x = Dropout(dropout, name=f\"dropout_{conv_level}\")(x)\n",
    "    if last_pooling == \"avg\":\n",
    "        x = GlobalAveragePooling2D(name=f\"lp_{last_pooling}\")(x)\n",
    "    elif last_pooling == \"max\":\n",
    "        x = GlobalMaxPooling2D(name=f\"lp_{last_pooling}\")(x)\n",
    "    x = Flatten(name=\"flatten\")(x)\n",
    "    if(numdense==2):\n",
    "        x=Dense(10,activation='relu',name='dense1')(x)\n",
    "    if last_dropout:\n",
    "        x = Dropout(last_dropout, name=\"last_dp\")(x)\n",
    "    output = Dense(1, activation='sigmoid', name=\"output\")(x)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics():\n",
    "    return [\n",
    "        AUC(name=\"auc\"),\n",
    "        BinaryAccuracy(\"accuracy\"),\n",
    "        CohenKappa(name=\"kappa\", num_classes=2)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model41avg = cnn_builder(starting_filters=32,\n",
    "                        conv_layers=4,\n",
    "                        convs_per_layer=1,\n",
    "                        pooling='avg',\n",
    "                        batch_norm=True,\n",
    "                        dropout=0.2,\n",
    "                        pool_strides=(2, 2),\n",
    "                        numdense=1)\n",
    "cnn_model41avg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model41max = cnn_builder(starting_filters=32,\n",
    "                        conv_layers=4,\n",
    "                        convs_per_layer=1,\n",
    "                        pooling='max',\n",
    "                        batch_norm=True,\n",
    "                        dropout=0.2,\n",
    "                        pool_strides=(2, 2),\n",
    "                        numdense=1)\n",
    "\n",
    "cnn_model41max.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model51avg = cnn_builder(starting_filters=32,\n",
    "                        conv_layers=5,\n",
    "                        convs_per_layer=1,\n",
    "                        pooling='avg',\n",
    "                        batch_norm=True,\n",
    "                        dropout=0.2,\n",
    "                        pool_strides=(2, 2),\n",
    "                        numdense=1)\n",
    "\n",
    "cnn_model51avg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model51max = cnn_builder(starting_filters=32,\n",
    "                        conv_layers=5,\n",
    "                        convs_per_layer=1,\n",
    "                        pooling='max',\n",
    "                        batch_norm=True,\n",
    "                        dropout=0.2,\n",
    "                        pool_strides=(2, 2),\n",
    "                        numdense=1)\n",
    "\n",
    "cnn_model51max.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('In the train dataset')\n",
    "for bp in df_train.body_part.unique():\n",
    "    sp0=df_train[(df_train.body_part==bp) & (df_train.label==0)].shape[0]\n",
    "    sp1=df_train[(df_train.body_part==bp) & (df_train.label==1)].shape[0]\n",
    "    print(f'{bp} 0: {sp0} ({100*sp0/(sp0+sp1): .2f}%)')\n",
    "    print(f'{bp} 1: {sp1} ({100*sp1/(sp0+sp1): .2f}%)')\n",
    "    print(f'{bp} all: {sp0+sp1} ({100*(sp0+sp1)/(df_train.shape[0]):.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('In the test dataset')\n",
    "for bp in df_test.body_part.unique():\n",
    "    sp0=df_test[(df_test.body_part==bp) & (df_test.label==0)].shape[0]\n",
    "    sp1=df_test[(df_test.body_part==bp) & (df_test.label==1)].shape[0]\n",
    "    print(f'{bp} 0: {sp0} ({100*sp0/(sp0+sp1):.2f}%)')\n",
    "    print(f'{bp} 1: {sp1} ({100*sp1/(sp0+sp1):.2f}%)')\n",
    "    print(f'{bp} all: {sp0+sp1} ({100*(sp0+sp1)/(df_test.shape[0]):.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train: {df_train.shape[0]} ({100*df_train.shape[0]/(df_train.shape[0]+df_test.shape[0]):.2f}%)')\n",
    "print(f'test: {df_test.shape[0]} ({100*df_test.shape[0]/(df_train.shape[0]+df_test.shape[0]):.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "sub_df = dataset_all.groupby(['body_part', 'label']).apply(lambda x: x.sample(1)).reset_index(drop = True)\n",
    "fig, (m_axs) = plt.subplots(2, sub_df.shape[0]//2, figsize = (12, 6))\n",
    "for c_ax, (_, c_row) in zip(m_axs.flatten(), sub_df.iterrows()):\n",
    "    c_ax.imshow(imread(c_row['img_path']), cmap = 'bone')\n",
    "    c_ax.axis('off')\n",
    "    c_ax.set_title('{body_part}:{label}'.format(**c_row))\n",
    "fig.savefig('samples.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv files path for\n",
    "path = 'MURA-v1.1'\n",
    "train_image_paths_csv = \"train_image_paths.csv\"\n",
    "train_images_paths = pd.read_csv(os.path.join(path,train_image_paths_csv),dtype=str,header=None)\n",
    "train_images_paths.columns = ['image_path']\n",
    "train_images_paths['label'] = train_images_paths['image_path'].map(lambda x:'positive' if 'positive' in x else 'negative')\n",
    "train_images_paths['category']  = train_images_paths['image_path'].apply(lambda x: x.split('/')[2])  \n",
    "train_images_paths['patientId']  = train_images_paths['image_path'].apply(lambda x: x.split('/')[3].replace('patient',''))\n",
    "train_images_paths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'MURA-v1.1'\n",
    "valid_image_paths_csv = \"valid_image_paths.csv\"\n",
    "valid_data_paths = pd.read_csv(os.path.join(path,valid_image_paths_csv),dtype=str,header=None)\n",
    "valid_data_paths.columns = ['image_path']\n",
    "valid_data_paths['label'] = valid_data_paths['image_path'].map(lambda x:'positive' if 'positive' in x else 'negative')\n",
    "valid_data_paths['category']  = valid_data_paths['image_path'].apply(lambda x: x.split('/')[2])  \n",
    "valid_data_paths['dir'] =  valid_data_paths['image_path'].apply(lambda x: x.split('/')[1])\n",
    "valid_data_paths['patientId']  = valid_data_paths['image_path'].apply(lambda x: x.split('/')[3].replace('patient',''))\n",
    "valid_data_paths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_paths['label_index']= train_images_paths.label\n",
    "train_images_paths.label_index.replace('positive', 1, inplace=True)\n",
    "train_images_paths.label_index.replace('negative', 0, inplace=True)\n",
    "train_images_paths.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_paths['label_index']= valid_data_paths.label\n",
    "valid_data_paths.label_index.replace('positive', 1, inplace=True)\n",
    "valid_data_paths.label_index.replace('negative', 0, inplace=True)\n",
    "valid_data_paths.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 256\n",
    "def random_rotation_flip(image,size = 256):\n",
    "    if random.randint(0,1):\n",
    "        image = cv2.flip(image,1) # 1-->horizontal flip 0-->Vertical flip -1-->Horizontal and vertical\n",
    "\n",
    "    if random.randint(0,1):\n",
    "            angle = random.randint(-30,30)\n",
    "            M = cv2.getRotationMatrix2D((size/2,size/2),angle,1)\n",
    "            #The third parameter: the size of the transformed image\n",
    "            image = cv2.warpAffine(image,M,(size,size))\n",
    "    return image\n",
    "def image_loader(Path, size = 224): \n",
    "    Images = []\n",
    "    for path in tqdm(Path):\n",
    "        try:\n",
    "            image = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "            image = cv2.resize(image,(size,size))\n",
    "            image = random_rotation_flip(image,size)\n",
    "            Images.append(image)\n",
    "        except Exception as e:\n",
    "            print(str(e))   \n",
    "    Images = np.asarray(Images).astype('float32')\n",
    "    mean = np.mean(Images)\n",
    "    std = np.std(Images)\n",
    "    Images = (Images - mean) / std\n",
    "    \n",
    "    return Images\n",
    "X_train = image_loader(train_images_paths['image_path'][:50,],im_size)\n",
    "y_train = train_images_paths['label']\n",
    "Y_train = y_train.replace(\"positive\",1)\n",
    "Y_train = Y_train.replace(\"negative\",0)\n",
    "\n",
    "X_test = image_loader(valid_data_paths['image_path'][:50,],im_size)\n",
    "y_test = valid_data_paths['label']\n",
    "Y_test = y_test.replace(\"positive\",1)\n",
    "Y_test = Y_test.replace(\"negative\",0)\n",
    "\n",
    "train, valid = train_test_split(train_images_paths, test_size=0.2,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = valid_data_paths.drop(['dir'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator_settings = dict(\n",
    "                          rescale = 1. / 255,\n",
    "                          #samplewise_center = True,\n",
    "                          #samplewise_std_normalization = True\n",
    "                          #rotation_range = 5, \n",
    "                         )\n",
    "image_generator = ImageDataGenerator(**image_generator_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'MURA-v1.1'\n",
    "train_generator = image_generator.flow_from_dataframe(dataframe = train,directory = None,x_col = 'image_path',y_col = 'label_index',batch_size = 64,shuffle = True,class_mode = 'raw', target_size = (im_size, im_size),color_mode = 'rgb',interpolation='nearest',validate_filenames=False,seed=seed)\n",
    "valid_generator = image_generator.flow_from_dataframe(dataframe = valid,directory = None,x_col = 'image_path',y_col = 'label_index',batch_size = 64,shuffle = True,class_mode = 'raw',target_size = (im_size, im_size),color_mode = 'rgb',interpolation='nearest',validate_filenames=True,seed=seed)\n",
    "test_generator = image_generator.flow_from_dataframe(dataframe = test,directory = None,x_col = 'image_path',y_col = 'label_index',batch_size = 64,shuffle = False,class_mode = 'raw', target_size = (im_size, im_size),color_mode = 'rgb',interpolation='nearest', validate_filenames=True,seed=seed)\n",
    "CLASSES = 2\n",
    "input_shape = (im_size,im_size,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor=\"val_kappa\",mode=\"max\", patience=3,restore_best_weights=True)\n",
    "epochs=10\n",
    "model=cnn_model41max\n",
    "model.compile(optimizer=Adam(), loss= BinaryCrossentropy(from_logits=False),metrics=[metrics()])\n",
    "history=model.fit(train_generator,validation_data = valid_generator, epochs = epochs,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_builder(\n",
    "        pooling=\"max\", \n",
    "        shape=(256, 256, 3), \n",
    "        trainable_layers_after=None\n",
    "    ):\n",
    "    resNet = ResNet152V2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=shape,\n",
    "        pooling=pooling\n",
    "    )\n",
    "    if trainable_layers_after:\n",
    "        for layer in resNet.layers[:trainable_layers_after]:\n",
    "            layer.trainable = False\n",
    "    else:\n",
    "        resNet.trainable = False\n",
    "    prediction_layer = Dense(1, activation=\"sigmoid\",\n",
    "                                name=\"resnet_output_sigmoid\")\n",
    "    model = Sequential(\n",
    "        layers=[\n",
    "            resNet,\n",
    "            prediction_layer\n",
    "        ],\n",
    "        name=\"resnet\"\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "early_stop = EarlyStopping(monitor=\"kappa\",mode=\"min\", patience=3, restore_best_weights=True)\n",
    "resnet_model = resnet_builder(pooling='avg')\n",
    "resnet_model.summary()\n",
    "resnet_model.compile(optimizer=Adam(), loss= BinaryCrossentropy(from_logits=False),metrics=[metrics()])\n",
    "hs = resnet_model.fit(train_generator,validation_data = valid_generator, epochs = epochs,callbacks=[early_stop])\n",
    "print('Finished training.')\n",
    "print('------------------')\n",
    "resnet_model.summary()\n",
    "filename = 'resnet1511.h5'\n",
    "resnet_model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "early_stop = EarlyStopping(monitor=\"val_kappa\",mode=\"max\", patience=3, restore_best_weights=True)\n",
    "resnet_model = resnet_builder(pooling='max') #change between max and avg \n",
    "resnet_model.summary()\n",
    "resnet_model.compile(optimizer=Adam(), loss= BinaryCrossentropy(from_logits=False),metrics=[metrics()])\n",
    "hs = resnet_model.fit(train_generator,validation_data = valid_generator, epochs = epochs,callbacks=[early_stop])\n",
    "print('Finished training.')\n",
    "print('------------------')\n",
    "resnet_model.summary()\n",
    "filename = 'resnet1511.h5'\n",
    "resnet_model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.evaluate(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGGNET16_builder(\n",
    "        pooling=\"max\", \n",
    "        shape=(256, 256, 3), \n",
    "        trainable_layers_after=None\n",
    "    ):\n",
    "    VGGNET16 = VGG16(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=shape,\n",
    "        pooling=pooling\n",
    "    )\n",
    "    if trainable_layers_after:\n",
    "        for layer in VGGNET16.layers[:trainable_layers_after]:\n",
    "            layer.trainable = False\n",
    "    else:\n",
    "        VGGNET16.trainable = False\n",
    "    prediction_layer = Dense(1, activation=\"sigmoid\",name=\"VGGNET_output_sigmoid\")\n",
    "    model = Sequential(\n",
    "        layers=[\n",
    "            VGGNET16,\n",
    "            prediction_layer\n",
    "        ],\n",
    "        name=\"VGGNET16\"\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "early_stop = EarlyStopping(monitor=\"kappa\", mode=\"min\", patience=3, restore_best_weights=True)\n",
    "VGGNET_model16 = VGGNET16_builder(pooling='max')\n",
    "VGGNET_model16.summary\n",
    "VGGNET_model16.compile(optimizer=Adam(),loss= BinaryCrossentropy(from_logits=False),metrics=[metrics()])\n",
    "hs = VGGNET_model16.fit(train_generator,validation_data = valid_generator, epochs = epochs,callbacks=[early_stop])\n",
    "print('Finished training.')\n",
    "print('------------------')\n",
    "VGGNET_model16.summary()\n",
    "filename = 'vgg16.h5'\n",
    "VGGNET_model16.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGNET_model16.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet_builder(\n",
    "        pooling=\"avg\",\n",
    "        shape=(256, 256, 3),\n",
    "        trainable_layers_after=None,\n",
    "        mlp=[],\n",
    "        mlp_dropout=0.25,\n",
    "        nameNN=\"\",\n",
    "):\n",
    "    denseNet = DenseNet201(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=shape,\n",
    "        pooling=pooling\n",
    "    )\n",
    "    if trainable_layers_after:\n",
    "        for layer in denseNet.layers[:trainable_layers_after]:\n",
    "            layer.trainable = False\n",
    "    else:\n",
    "        denseNet.trainable = False\n",
    "    output = denseNet.output\n",
    "    for index, mlp_neurons in enumerate(mlp):\n",
    "        output = Dense(mlp_neurons, activation=\"relu\", name=f\"m.{index}.{mlp_neurons}\")(output)\n",
    "        if mlp_dropout:\n",
    "            output = Dropout(mlp_dropout, name=f\"mdp.{index}.{mlp_neurons}\")(output)\n",
    "    output = Dense(1, activation=\"sigmoid\", name=\"densenet_output_sigmoid\")(output)\n",
    "    model = Model(denseNet.input, output, name='densenet'+nameNN)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "early_stop = EarlyStopping(monitor=\"val_kappa\", mode=\"max\", patience=2,restore_best_weights=True)\n",
    "densenet_model = densenet_builder(pooling='avg')\n",
    "densenet_model.compile(optimizer=Adam(), \n",
    "                  loss= BinaryCrossentropy(from_logits=False),\n",
    "                  metrics=[metrics()])\n",
    "# hs = densenet_model.fit(\n",
    "#         train_generator,\n",
    "#         validation_data = valid_generator, \n",
    "#         epochs = epochs,\n",
    "#         callbacks=[early_stop]\n",
    "#     )\n",
    "print('Finished training.')\n",
    "print('------------------')\n",
    "densenet_model.evaluate(test_generator)\n",
    "filename = 'densenet.h5'\n",
    "densenet_model.save(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a6a06d477e5e32c82aaca5412c01e336c42bc456313ee105f674b72f8e001fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
